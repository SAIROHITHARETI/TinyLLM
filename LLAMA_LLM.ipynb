{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIrFCt54kBi0HFBulKH4E6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAIROHITHARETI/TinyLLM/blob/main/LLAMA_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# =====================================================================\n",
        "# 1. RMSNorm\n",
        "# =====================================================================\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = x.pow(2).mean(-1, keepdim=True).sqrt()\n",
        "        return self.weight * (x / (rms + self.eps))\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 2. RoPE Helpers\n",
        "# =====================================================================\n",
        "def apply_rope(x, cos, sin):\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "\n",
        "    cos = cos[..., : x1.size(-1)]\n",
        "    sin = sin[..., : x1.size(-1)]\n",
        "\n",
        "    out1 = x1 * cos - x2 * sin\n",
        "    out2 = x1 * sin + x2 * cos\n",
        "\n",
        "    out = torch.zeros_like(x)\n",
        "    out[..., ::2] = out1\n",
        "    out[..., 1::2] = out2\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_rope(freqs, T, device):\n",
        "    t = torch.arange(T, device=device)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    cos = freqs.cos()[None, :, None, :]\n",
        "    sin = freqs.sin()[None, :, None, :]\n",
        "    return cos, sin\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 3. GQA Attention with KV-Cache\n",
        "# =====================================================================\n",
        "class GQA(nn.Module):\n",
        "    def __init__(self, dim, n_heads, n_kv_heads):\n",
        "        super().__init__()\n",
        "        assert dim % n_heads == 0\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.head_dim = dim // n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(dim, dim)\n",
        "        self.k_proj = nn.Linear(dim, self.head_dim * n_kv_heads)\n",
        "        self.v_proj = nn.Linear(dim, self.head_dim * n_kv_heads)\n",
        "        self.o_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, cos, sin, kv_cache=None):\n",
        "        B, T, D = x.shape\n",
        "\n",
        "        # Project q/k/v\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim)\n",
        "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.head_dim)\n",
        "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        # RoPE\n",
        "        q = apply_rope(q, cos, sin)\n",
        "        k = apply_rope(k, cos, sin)\n",
        "\n",
        "        # Expand KV heads\n",
        "        repeat = self.n_heads // self.n_kv_heads\n",
        "        k = k.repeat_interleave(repeat, dim=2)\n",
        "        v = v.repeat_interleave(repeat, dim=2)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        #   KV CACHE\n",
        "        # ---------------------------------------------------------\n",
        "        if kv_cache is not None:\n",
        "            prev_k, prev_v = kv_cache\n",
        "            k = torch.cat([prev_k, k], dim=1)\n",
        "            v = torch.cat([prev_v, v], dim=1)\n",
        "\n",
        "        new_cache = (k, v)  # save for next step\n",
        "\n",
        "        # Attention scores\n",
        "        att = torch.einsum(\"bthd,bThd->bhtT\", q, k) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Causal mask only needed when T > 1\n",
        "        if kv_cache is None:\n",
        "            causal = torch.tril(torch.ones(att.size(-1), att.size(-1), device=x.device))\n",
        "            att = att.masked_fill(causal == 0, float('-inf'))\n",
        "\n",
        "        att = torch.softmax(att, dim=-1)\n",
        "\n",
        "        out = torch.einsum(\"bhtT,bThd->bthd\", att, v)\n",
        "        out = out.reshape(B, T, D)\n",
        "\n",
        "        return self.o_proj(out), new_cache\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 4. Transformer Block\n",
        "# =====================================================================\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, n_kv_heads, mlp_ratio=4):\n",
        "        super().__init__()\n",
        "        self.norm1 = RMSNorm(dim)\n",
        "        self.attn = GQA(dim, n_heads, n_kv_heads)\n",
        "        self.norm2 = RMSNorm(dim)\n",
        "\n",
        "        hidden = dim * mlp_ratio\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cos, sin, kv_cache=None):\n",
        "        att_out, new_cache = self.attn(self.norm1(x), cos, sin, kv_cache)\n",
        "        x = x + att_out\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, new_cache\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5. MiniLLaMA Model with KV-cache Support\n",
        "# =====================================================================\n",
        "class MiniLLaMA(nn.Module):\n",
        "    def __init__(self, vocab_size, dim=128, depth=3, n_heads=4, n_kv_heads=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2) / dim))\n",
        "        self.register_buffer(\"freqs\", freqs)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(dim, n_heads, n_kv_heads) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = RMSNorm(dim)\n",
        "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, kv_cache=None):\n",
        "        B, T = idx.shape\n",
        "        x = self.embed(idx)\n",
        "\n",
        "        cos, sin = build_rope(self.freqs, T, x.device)\n",
        "\n",
        "        new_cache = []\n",
        "\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            block_cache = kv_cache[i] if kv_cache is not None else None\n",
        "            x, updated = blk(x, cos, sin, block_cache)\n",
        "            new_cache.append(updated)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits, new_cache\n",
        "\n"
      ],
      "metadata": {
        "id": "KyLgA0zc7Jhn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# 6. Tiny Dataset (\"abcabcabc\")\n",
        "# =====================================================================\n",
        "text = \"abcdefghijklmnopqrstuvwxyz\" * 5\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "vocab_size = len(chars)\n",
        "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 7. Mini Training Loop\n",
        "# =====================================================================\n",
        "model = MiniLLaMA(vocab_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def get_batch(seq_len=8):\n",
        "    i = torch.randint(0, len(data) - seq_len - 1, (1,))\n",
        "    x = data[i:i + seq_len]\n",
        "    y = data[i + 1:i + seq_len + 1]\n",
        "    return x.unsqueeze(0), y.unsqueeze(0)\n",
        "\n",
        "print(\"Training...\")\n",
        "for step in range(1000):\n",
        "    x, y = get_batch()\n",
        "\n",
        "    logits, _ = model(x)\n",
        "    loss = nn.functional.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"step {step}: loss={loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5QbXN3eGrEb",
        "outputId": "587a4dfe-e2b3-4a47-ac1b-fb3fe1401902"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "step 0: loss=3.3483\n",
            "step 100: loss=0.0135\n",
            "step 200: loss=0.0063\n",
            "step 300: loss=0.0036\n",
            "step 400: loss=0.0025\n",
            "step 500: loss=0.0021\n",
            "step 600: loss=0.0015\n",
            "step 700: loss=0.0012\n",
            "step 800: loss=0.0008\n",
            "step 900: loss=0.0007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# 8. Autoregressive Generation with KV-cache\n",
        "# =====================================================================\n",
        "def generate(model, start, length=25):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([[stoi[ch] for ch in start]], dtype=torch.long)\n",
        "    kv_cache = None\n",
        "\n",
        "    for step in range(length):\n",
        "        logits, kv_cache = model(idx[:, -1:], kv_cache)\n",
        "\n",
        "        # print(f\"\\n=== Generation Step {step+1} ===\")\n",
        "        # for i, (k, v) in enumerate(kv_cache):\n",
        "        #     print(f\"Block {i}: k = {k.shape}, v = {v.shape}\")\n",
        "\n",
        "        next_id = torch.argmax(logits[:, -1], dim=-1)\n",
        "        idx = torch.cat([idx, next_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "    return \"\".join(itos[i.item()] for i in idx[0])"
      ],
      "metadata": {
        "id": "YTxNqkntGkkJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerated:\")\n",
        "print(generate(model, \"b\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub8Z5dTH7QEF",
        "outputId": "5ee6cd24-8c20-44fa-b418-a5b5b4805882"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated:\n",
            "bcdefghijklmnopqrstuvwxyza\n"
          ]
        }
      ]
    }
  ]
}